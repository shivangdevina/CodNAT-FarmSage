# -*- coding: utf-8 -*-
"""rag_backend.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kVia6ZGTJxNrv4O-guHlXWOZZ60ra1dc
"""

CFG = {
    "gemini_model": "gemini-1.5-flash",
    "gemini_api_key": "AIzaSyA1ro1wHSrhPImwH0SRfs9RLf9MWrYM1Ts",
    "temperature": 0.9
}

# !pip install python-docx
# !pip install pytesseract
# !pip install pdf2image
# !pip install docling
# !pip install pillow
# !sudo apt-get install poppler-utils
# !pip install pypdf
# !pip install -U duckduckgo-search
# !pip install langchain-community
# !pip install nltk
# !pip install sentence-transformers
# !pip install chromadb
# !pip install langchain-community
# !pip install langchain-google-genai

# pip: python-docx
# pip: pytesseract
# pip: pdf2image
# pip: docling
# pip: pillow

import logging
from pdf2image import convert_from_path
from pathlib import Path
from typing import List, Dict, Any, Optional
from PIL import Image
from dataclasses import dataclass
# from docling.document_converter import DocumentConverter # Remove this import
from pypdf import PdfReader

logger = logging.getLogger("rag")

@dataclass
class Document:
    id: str
    uri: str
    metadata: Dict[str, Any]
    text: Optional[str] = None
    blob_path: Optional[Path] = None

def collect_documents(data_dir: Path) -> List[Document]:
    docs: List[Document] = []
    for file_path in data_dir.glob("**/*"):
        if file_path.suffix.lower() in [".pdf", ".docx", ".txt", ".md"]:
            doc = Document(
                id=file_path.stem,
                uri=str(file_path),
                metadata={},
                blob_path=file_path
            )
            docs.append(doc)
    logger.info(f"Collected {len(docs)} documents from {data_dir}")
    return docs

def apply_ocr_if_needed(doc: Document, cfg: Dict[str, Any]) -> str:
    if doc.text:
        return doc.text
    if doc.blob_path.suffix.lower() == ".pdf":
        images = convert_from_path(doc.blob_path)
        text = "\n".join(pytesseract.image_to_string(img) for img in images)
        return text.strip()
    return ""

# Removed parse_with_docling function

def extract_metadata(text: str) -> Dict[str, Any]:
    metadata: Dict[str, Any] = {}
    lowered = text.lower()
    for crop in ["wheat", "rice", "maize", "cotton", "sugarcane"]:
        if crop in lowered:
            metadata["crop"] = crop
            break
    for season in ["kharif", "rabi", "summer"]:
        if season in lowered:
            metadata["season"] = season
            break
    for pest in ["bollworm", "stem borer", "aphid", "locust"]:
        if pest in lowered:
            metadata["pest"] = pest
            break
    return metadata

def ingest_and_parse_documents(data_dir: Path, cfg: Dict[str, Any]) -> List[Document]:
    documents = collect_documents(data_dir)
    for doc in documents:
        text = ""
        if doc.blob_path.suffix.lower() == ".txt":
            text = doc.blob_path.read_text(encoding="utf-8")
        elif doc.blob_path.suffix.lower() == ".pdf":
            try:
                reader = PdfReader(doc.blob_path)
                pdf_text = ""
                for page in reader.pages:
                    pdf_text += page.extract_text() or ""
                text = pdf_text.strip()
                if not text:
                    logger.warning(f"pypdf extracted no text from {doc.id}, attempting OCR.")
                    text = apply_ocr_if_needed(doc, cfg)
            except Exception as e:
                logger.warning(f"pypdf parsing failed for {doc.id}: {e}, attempting OCR.")
                # Fallback directly to OCR if pypdf fails
                text = apply_ocr_if_needed(doc, cfg)
        # Removed the Docling parsing for other file types as it's no longer required for this specific task.
        # If handling .docx, .md etc. is still needed, a separate parser would be required.
        # For this task, we only focus on PDF and TXT, with OCR fallback for PDF.


        doc.text = text
        if text:
            doc.metadata.update(extract_metadata(text))
    logger.info(f"Ingested and parsed {len(documents)} documents")
    return documents

import nltk
from typing import List
#from datetime import datetime
from datetime import datetime, timezone
from dataclasses import dataclass
from typing import Optional # Import Optional

nltk.download("punkt", quiet=True)

@dataclass
class Chunk:
    doc_id: str
    chunk_id: str
    text: str
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None # Add optional embedding field


def chunk_documents(documents: List[Document], chunk_size: int = 500, overlap: int = 50) -> List[Chunk]:
    chunks: List[Chunk] = []
    for doc in documents:
        if not doc.text:
            continue
        words = doc.text.split()
        start = 0
        chunk_index = 0
        while start < len(words):
            end = start + chunk_size
            text_segment = " ".join(words[start:end])
            chunk_metadata = {
                "doc_id": doc.id,
                "uri": doc.uri,
                "source": doc.metadata.get("source"),
                "page": doc.metadata.get("page", None),
                #"timestamp": datetime.utcnow().isoformat()
                "timestamp": datetime.datetime.now(timezone.utc).isoformat(),

            }
            chunk = Chunk(
                doc_id=doc.id,
                chunk_id=f"{doc.id}_{chunk_index}",
                text=text_segment,
                metadata=chunk_metadata
            )
            chunks.append(chunk)
            chunk_index += 1
            start += chunk_size - overlap
    logger.info(f"Created {len(chunks)} chunks from {len(documents)} documents")
    return chunks

import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any
import hashlib
import time
from pathlib import Path
from langchain_community.vectorstores import Chroma
from langchain_core.embeddings import Embeddings

# Stable directories
DATA_DIR = Path("data")
CACHE_DIR = Path("cache")
OUTPUT_DIR = Path("output")
CACHE_DIR.mkdir(parents=True, exist_ok=True)
DATA_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Initialize embedding model
# Wrap SentenceTransformer with a class that has an embed_documents method
class SentenceTransformerEmbeddings(Embeddings):
    def __init__(self, model_name: str):
        self.model = SentenceTransformer(model_name)

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self.model.encode(texts, batch_size=32, show_progress_bar=False).tolist()

    def embed_query(self, text: str) -> List[float]:
        return self.model.encode([text], show_progress_bar=False).tolist()[0]

EMB = SentenceTransformerEmbeddings("sentence-transformers/all-MiniLM-L6-v2")


def get_embedding_version(model_name: str) -> str:
    # Use the class name for versioning
    return hashlib.sha256(model_name.encode("utf-8")).hexdigest()[:8]

def generate_embeddings(chunks: List[Chunk], collection_name: str = "agri_docs") -> Chroma:
    # Use LangChain's Chroma.from_documents to create and populate the collection
    # Note: This function will also handle the initialization of the Chroma client internally
    # or use an existing one if passed via client argument.
    # For persistence, ensure the persist_directory is set in the from_documents call.

    # Convert custom Chunk objects to LangChain Document objects
    from langchain.schema import Document as LCDocument
    lc_documents = [LCDocument(page_content=chunk.text, metadata=chunk.metadata) for chunk in chunks]

    # Generate embeddings and add to Chroma
    vectorstore = Chroma.from_documents(
        documents=lc_documents,
        embedding=EMB,
        collection_name=collection_name,
        persist_directory=str(CACHE_DIR / "chroma") # Ensure persistence
    )

    logger.info(f"Generated and stored {len(chunks)} embeddings in collection '{collection_name}' using LangChain")
    return vectorstore

# pip: langchain-google-genai

from langchain_google_genai import ChatGoogleGenerativeAI
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

# Define dataclasses used in this cell
@dataclass
class Query:
    id: str
    text: str
    metadata: Dict[str, Any]

@dataclass
class Chunk:
    doc_id: str
    chunk_id: str
    text: str
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None # Add optional embedding field

@dataclass
class Retrieval:
    query_id: str
    chunks: List[Chunk]
    metadata: Dict[str, Any]


# -----------------------------
# Config dictionary (replace API key)
# -----------------------------
CFG = {
    "gemini_model": "gemini-1.5-flash",
    "gemini_api_key": "AIzaSyAOEJ-dOSCWANivILtskRf9j0mta3CZSVQ",
    "temperature": 0.6
}

# Initialize Gemini LLM via LangChain wrapper
LLM = ChatGoogleGenerativeAI(
    model=CFG["gemini_model"],
    google_api_key=CFG["gemini_api_key"],
    temperature=CFG["temperature"]
)

def hyde_expand(query: Query) -> str:
    prompt = (
        f"Generate a hypothetical detailed answer for the following agricultural question:\n\n"
        f"{query.text}\n\nAnswer:"
    )
    response = LLM.invoke(prompt)
    return response.content.strip()

def generate_query_embeddings(texts: List[str]) -> List[List[float]]:
    # Use EMB.embed_documents for multiple texts or a loop with EMB.embed_query
    # As the function is called with a list of texts, embed_documents is appropriate
    return EMB.embed_documents(texts)

def multi_query_expand(query: Query, num_variants: int = 3) -> List[str]:
    prompt = (
        f"Generate {num_variants} diverse rephrasings of the following agricultural query:\n\n"
        f"{query.text}\n\nRephrasings:"
    )
    response = LLM.invoke(prompt)
    variants = [line.strip("- ").strip() for line in response.content.split("\n") if line.strip()]
    return variants[:num_variants]

def retrieve_with_expansion(query: Query, collection_name: str = "agri_docs", top_k: int = 5) -> Retrieval:
    # retrieve_with_expansion requires VSTORE and EMB to be initialized globally
    if 'VSTORE' not in globals():
         raise ValueError("Chroma persistent client (VSTORE) is not initialized.")
    if 'EMB' not in globals():
         raise ValueError("Embedding model (EMB) is not initialized.")

    collection = VSTORE.get_collection(name=collection_name)

    # Original query retrieval
    # Use embed_query for a single query text
    orig_emb = EMB.embed_query(query.text)
    orig_results = collection.query(query_embeddings=[orig_emb], n_results=top_k)

    # HyDE retrieval
    hyde_text = hyde_expand(query)
    hyde_emb = EMB.embed_query(hyde_text)
    hyde_results = collection.query(query_embeddings=[hyde_emb], n_results=top_k)

    # Multi-query retrieval
    expansions = multi_query_expand(query)
    # Use embed_documents for a list of texts
    exp_embs = generate_query_embeddings(expansions)
    exp_results = collection.query(query_embeddings=exp_embs, n_results=top_k)

    # Aggregate unique chunks
    all_chunks: List[Chunk] = []
    seen_ids = set()
    # Combine results from all sources, giving each source a label in metadata for RRF
    combined_results = defaultdict(list)

    if orig_results and orig_results.get("ids"):
        for i in range(len(orig_results["ids"][0])):
            chunk_id = orig_results["ids"][0][i]
            text = orig_results["documents"][0][i]
            metadata = orig_results["metadatas"][0][i]
            metadata["source"] = "original" # Label source
            combined_results[chunk_id].append((text, metadata))


    if hyde_results and hyde_results.get("ids"):
        for i in range(len(hyde_results["ids"][0])):
            chunk_id = hyde_results["ids"][0][i]
            text = hyde_results["documents"][0][i]
            metadata = hyde_results["metadatas"][0][i]
            metadata["source"] = metadata.get("source", "") + ",hyde" # Add hyde source
            combined_results[chunk_id].append((text, metadata))


    if exp_results and exp_results.get("ids"):
        # Flatten the list of lists from query_embeddings with multiple queries
        flat_ids = [item for sublist in exp_results["ids"] for item in sublist]
        flat_documents = [item for sublist in exp_results["documents"] for item in sublist]
        flat_metadatas = [item for sublist in exp_results["metadatas"] for item in sublist]

        for i in range(len(flat_ids)):
            chunk_id = flat_ids[i]
            text = flat_documents[i]
            metadata = flat_metadatas[i]
            metadata["source"] = metadata.get("source", "") + ",expanded" # Add expanded source
            combined_results[chunk_id].append((text, metadata))


    # Create Chunk objects, prioritizing metadata from the first source it appeared in
    unique_chunks: List[Chunk] = []
    for chunk_id, results_list in combined_results.items():
        # Use the text and metadata from the first appearance
        first_result = results_list[0]
        text, metadata = first_result
        # Ensure source field is a string or list consistently
        if isinstance(metadata.get("source"), list):
            metadata["source"] = ",".join(metadata["source"])


        chunk = Chunk(
            doc_id=metadata.get("doc_id", ""),
            chunk_id=chunk_id,
            text=text,
            metadata=metadata
            # Embedding is not retrieved here, will be generated in CRAG if needed
        )
        unique_chunks.append(chunk)

    # Apply RRF to the unique chunks based on their simulated ranks from different sources
    # Need to pass the query text to RRF if rank calculation depends on relevance (not currently the case for RRF)
    # RRF function (in e9DJSG5M2JEe) needs to be updated to handle the "source" metadata correctly
    # and ideally receive the initial retrieval results with ranks for each source.

    # For now, let's return the unique chunks and apply RRF in the pipeline after retrieval.
    # The current RRF implementation in e9DJSG5M2JEe seems to expect a Retrieval object
    # containing all aggregated chunks and uses the "source" metadata to simulate ranks.
    # Let's return the unique chunks in a Retrieval object and call RRF after this function.

    retrieval = Retrieval(
        query_id=query.id,
        chunks=unique_chunks,
        metadata={"expansions": expansions, "hyde_text": hyde_text}
    )

    logger.info(
        f"Retrieved and aggregated {len(unique_chunks)} unique chunks for query {query.id} "
        f"from original, HyDE, and expanded queries"
    )
    return retrieval

import math
from collections import defaultdict
from typing import List

def reciprocal_rank_fusion(retrieval: Retrieval, k: int = 60) -> Retrieval:
    """
    Apply Reciprocal Rank Fusion (RRF) to rerank chunks from different retrieval sources.
    - retrieval: Retrieval object containing aggregated chunks (from orig, HyDE, expansions).
    - k: constant to control the influence of rank position.
    """
    # Each chunk may appear in multiple sources with different ranks → fuse scores
    score_dict = defaultdict(float)
    print("NO OF CHUNKS ENTERNIG THE RRF: ")
    print(len(retrieval.chunks))
    # Simulate separate retrieval sets (orig, hyde, expanded)
    # Metadata keeps track of where each chunk came from
    sources = ["original", "hyde", "expanded"]

    for source in sources:
        ranked_chunks = [
            (idx, chunk) for idx, chunk in enumerate(retrieval.chunks)
            if source in chunk.metadata.get("source", "original")  # default "all"
        ]
        print(f"RRF: ranked_chunks for source '{source}': {len(ranked_chunks)} chunks") # Diagnostic print
        if ranked_chunks:
             print(f"RRF: First ranked chunk for source '{source}': ID={ranked_chunks[0][1].chunk_id}, Source Metadata={ranked_chunks[0][1].metadata.get('source')}") # Diagnostic print

        for rank, chunk in ranked_chunks:
            score_dict[chunk.chunk_id] += 1.0 / (k + rank + 1)

    # Attach scores back to chunks
    chunk_map = {chunk.chunk_id: chunk for chunk in retrieval.chunks}
    reranked_chunks = sorted(
        [(cid, score) for cid, score in score_dict.items()],
        key=lambda x: x[1],
        reverse=True
    )

    final_chunks: List[Chunk] = []
    seen_ids = set()
    for cid, score in reranked_chunks:
        if cid not in seen_ids:
            chunk = chunk_map[cid]
            chunk.metadata["fusion_score"] = score
            final_chunks.append(chunk)
            seen_ids.add(cid)

    fused_retrieval = Retrieval(
        query_id=retrieval.query_id,
        chunks=final_chunks,
        metadata={
            "fusion_method": "RRF",
            "expansions": retrieval.metadata.get("expansions", []),
            "hyde_text": retrieval.metadata.get("hyde_text", "")
        }
    )

    logger.info(
        f"RRF fusion complete for query {retrieval.query_id}: "
        f"{len(final_chunks)} final unique chunks"
    )
    return fused_retrieval

import datetime
import math
from collections import defaultdict
from typing import List, Tuple # Import Tuple



def crag_scoring(
    retrieval: Retrieval,
    query: str,
    llm: ChatGoogleGenerativeAI,
    recency_weight: float = 0.3,
    relevance_weight: float = 0.5,
    provenance_weight: float = 0.2,
    threshold: float = 0.65,
) -> Tuple[str, Retrieval]:
    """
    Evaluate chunks using CRAG (Content relevance + Recency + Provenance).
    Returns: ("modular" | "agentic", scored_retrieval)
    """
    # Ensure EMB is accessible globally
    if 'EMB' not in globals():
        logger.error("Embedding model (EMB) is not initialized for CRAG scoring.")
        # Handle this error appropriately - maybe raise an exception or return a default
        return "error", Retrieval(query_id=retrieval.query_id, chunks=[], metadata={"error": "Embedding model not initialized"})


    now = datetime.datetime.utcnow()
    print("no of chunks retrieved: ")
    print(len(retrieval.chunks))
    scored_chunks = []
    for chunk in retrieval.chunks:
        # --- Content relevance (semantic match via Gemini embedding similarity) ---
        # Need to ensure embedding_model is accessible in this scope or passed in
        # Assuming EMB is a global variable initialized elsewhere (e.g., zPb2lJcqzkaJ)

        query_embedding = EMB.embed_query(query)
        # Ensure chunk has an embedding
        if chunk.embedding is None:
             # Generate embedding for the chunk if missing
             try:
                  chunk.embedding = EMB.embed_query(chunk.text)
                  relevance = cosine_similarity(query_embedding, chunk.embedding)
             except Exception as e:
                  logger.warning(f"Error generating embedding for chunk {chunk.chunk_id}: {e}")
                  relevance = 0.0 # Default to low relevance on error

        else:
            chunk_embedding = chunk.embedding
            relevance = cosine_similarity(query_embedding, chunk_embedding)


        # --- Recency (favor newer documents) ---
        ts = chunk.metadata.get("timestamp")
        if ts:
            try:
                # Parse timestamp with timezone information
                ts_dt = datetime.datetime.fromisoformat(ts)
                # Ensure both 'now' and 'ts_dt' are timezone-aware for comparison
                if ts_dt.tzinfo is None:
                     # Assume UTC if no timezone info (based on how it's created in chunk_documents)
                     ts_dt = ts_dt.replace(tzinfo=datetime.timezone.utc)
                # Ensure 'now' is also timezone-aware (utcnow is timezone-aware)
                if now.tzinfo is None:
                     now = now.replace(tzinfo=datetime.timezone.utc)

                age_days = max((now - ts_dt).days, 1)
                recency = 1 / math.log(age_days + 2)
            except ValueError as e:
                logger.warning(f"Could not parse timestamp '{ts}' for chunk {chunk.chunk_id}: {e}")
                recency = 0.5 # Neutral score on parsing error
            except Exception as e:
                 logger.warning(f"Error calculating recency for chunk {chunk.chunk_id}: {e}")
                 recency = 0.5 # Neutral score on other errors

        else:
            recency = 0.5  # neutral if no timestamp

        # --- Provenance (trusted sources get higher score) ---
        source = chunk.metadata.get("source", "unknown")
        if source in ["gov", "research", "university"]:
            provenance = 1.0
        elif source in ["manual", "guide"]:
            provenance = 0.7
        else:
            provenance = 0.4

        # --- Weighted CRAG Score ---
        crag_score = (
            relevance_weight * relevance
            + recency_weight * recency
            + provenance_weight * provenance
        )

        chunk.metadata["crag_score"] = crag_score
        scored_chunks.append(chunk)

    # Sort by CRAG score
    scored_chunks = sorted(scored_chunks, key=lambda c: c.metadata["crag_score"], reverse=True)

    # Decide RAG path
    top_score = scored_chunks[0].metadata["crag_score"] if scored_chunks else 0.0
    if top_score >= threshold:
        decision = "modular"
    else:
        decision = "agentic"

    final_retrieval = Retrieval(
        query_id=retrieval.query_id,
        chunks=scored_chunks,
        metadata={
            "decision": decision,
            "threshold": threshold,
            "query": query,
        },
    )

    logger.info(
        f"CRAG scoring complete: Decision = {decision.upper()}, "
        f"Top score = {top_score:.3f}, Threshold = {threshold}"
    )
    return decision, final_retrieval


def cosine_similarity(vec1, vec2):
    """Basic cosine similarity for embeddings"""
    dot = sum(a * b for a in vec1 for b in vec2) # Fixed dot product calculation
    norm1 = math.sqrt(sum(a * a for a in vec1))
    norm2 = math.sqrt(sum(b * b for b in vec2))
    return dot / (norm1 * norm2 + 1e-8)



from langchain.agents import initialize_agent, AgentType, Tool
# Remove previous web search tool imports
# from langchain_perplexity.retrievers import PerplexityRetriever
# from langchain_community.tools import BraveSearch
# from langchain_community.tools import SearxSearchResults
# from langchain_community.tools.google_search import GoogleSearch # Incorrect import
# from langchain_community.utilities import GoogleSearch # Incorrect import
# Remove custom Google Search implementation using google-api-python-client
# from googleapiclient.discovery import build
# from googleapiclient.errors import HttpError # Import HttpError for error handling

# install necessary libraries


from langchain_community.vectorstores import Chroma
from langchain_core.embeddings import Embeddings
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import logging
from collections import defaultdict # Import defaultdict
# Remove userdata import as we will be directly pasting keys
# from google.colab import userdata # Import userdata to access secrets

# Import necessary components for Google Search using GoogleSearchAPIWrapper
from langchain_community.utilities import GoogleSearchAPIWrapper
from langchain_core.tools import Tool as LangchainTool # Import Tool from langchain_core and alias it


logger = logging.getLogger("rag")


# Define dataclasses used in this cell (if not already defined in previous cells)
# Assuming Document and Chunk dataclasses are defined in previous cells
@dataclass
class Retrieval:
    query_id: str
    chunks: List[Chunk]
    metadata: Dict[str, Any]

# --- Define Tools for Agentic RAG ---
# The retriever tool needs a function that takes a query string and returns documents
def get_relevant_documents(query: str) -> List[Document]:
    """Use this tool to search internal knowledge base (agriculture manuals, guides, research)."""
    # Assuming 'vectorstore' is a global variable initialized elsewhere (e.g., xtTL4IXVZNDZ)
    # and it's a LangChain Chroma object.
    if 'vectorstore' not in globals() or not isinstance(vectorstore, Chroma):
        raise ValueError("Vector store is not initialized or is not a LangChain Chroma object.")
    logger.info(f"[Agentic RAG - Retriever Tool] Searching vector store for query: {query}")
    # LangChain's similarity_search returns LangChain Document objects, not our custom Document dataclass
    # Need to ensure this is handled correctly downstream or convert if necessary.
    # For now, assume downstream can handle LangChain Document objects.
    # Limit the number of results from the vector store
    return vectorstore.similarity_search(query, k=5) # Limit to top 5 results


retriever_tool = Tool(
    name="VectorRetriever",
    func=get_relevant_documents,
    description="Use this tool to search internal knowledge base (agriculture manuals, guides, research)."
)

# --- Google Search Tool using GoogleSearchAPIWrapper ---

# IMPORTANT: Pasting API keys directly into your code is not recommended for security.
# Use Colab secrets (🔑 icon on the left) for sensitive information.
# Replace the placeholder values below with your actual Google API Key and CSE ID.
google_api_key = "AIzaSyAy-uDVGvJINXTo0gvIRncOJtEm-pM5ues"
google_cse_id = "0436a46c86379439f"


# Initialize the wrapper; it uses env vars GOOGLE_API_KEY and GOOGLE_CSE_ID
# Ensure GOOGLE_API_KEY and GOOGLE_CSE_ID are set as environment variables or passed to the wrapper
# By default, GoogleSearchAPIWrapper looks for GOOGLE_API_KEY and GOOGLE_CSE_ID env vars.
# If you paste them directly as variables like above, you might need to set them as env vars
# or modify the wrapper initialization if it supports direct parameter passing.
# Let's stick to setting env vars from the variables for this approach.
import os
os.environ["GOOGLE_API_KEY"] = google_api_key
os.environ["GOOGLE_CSE_ID"] = google_cse_id

search = GoogleSearchAPIWrapper()

# Wrap as a Tool
web_search_tool = LangchainTool( # Use LangchainTool alias to avoid conflict with agent Tool import
    name="WebSearch", # Renamed to match the intended tool name in the agent
    description="Search Google for recent results.",
    func=search.run
)


# The summarizer tool should take a list of documents and return a summary string
def summarize_documents(docs: List[Document]) -> str:
    """Summarize retrieved documents into concise evidence."""
    # Assuming 'LLM' is a global variable initialized elsewhere (e.g., Hm5v3n3R14nY)
    if 'LLM' not in globals():
         raise ValueError("LLM is not initialized.")
    logger.info(f"[Agentic RAG - Summarizer Tool] Summarizing {len(docs)} documents.")
    # Concatenate document content for summarization
    # Ensure 'docs' contains objects with a 'page_content' attribute (like LangChain Document)
    text_to_summarize = " ".join([getattr(doc, 'page_content', str(doc)) for doc in docs])
    # Use the LLM to summarize
    prompt = "Summarize the following text:\n\n" + text_to_summarize
    # Limit the text length to avoid overwhelming the LLM context window
    max_summary_length = 4000 # Adjust based on LLM's context window and expected summary length
    if len(prompt) > max_summary_length:
        prompt = prompt[:max_summary_length] + "..." # Truncate if too long


    try:
        response = LLM.invoke(prompt)
        return response.content.strip()
    except Exception as e:
        logger.error(f"[Agentic RAG - Summarizer Tool] LLM summarization failed: {e}")
        return "Could not summarize the retrieved information."


summarizer_tool = Tool(
    name="Summarizer",
    func=summarize_documents,
    description="Summarize retrieved documents into concise evidence."
)

tools = [retriever_tool, web_search_tool, summarizer_tool]

# --- Initialize LangChain Agent ---
# Ensure LLM is accessible globally
if 'LLM' not in globals():
     raise ValueError("LLM is not initialized for agent.")

# Note: Initializing the agent here makes it a global variable 'agent'.
# This agent uses the defined tools and the global LLM.
agent = initialize_agent(
    tools=tools,
    llm=LLM,  # Gemini LLM already wrapped with LangChain
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
    handle_parsing_errors=True # Add error handling for agent parsing
)

def modular_rag(query: str, retrieval: Retrieval, chat_history: List[str]) -> str:
    """
    Generates an answer based on the provided retrieval chunks (Modular RAG).
    """
    logger.info(f"[Modular RAG] Processing query: {query}")
    if not retrieval or not retrieval.chunks:
        logger.warning("[Modular RAG] No retrieval chunks provided.")
        return "I could not find relevant information in the knowledge base."

    # Concatenate the text from the top chunks to provide context to the LLM
    # You might want to select a certain number of chunks or chunks above a certain score
    # For simplicity, let's use the top few chunks based on their CRAG score (already sorted by CRAG)
    context_chunks = retrieval.chunks[:5] # Use top 5 chunks as context
    context = "\n\n".join([f"--- Source: {c.metadata.get('doc_id', 'Unknown')} ---\n{c.text}" for c in context_chunks])

    # Incorporate chat history into the prompt
    history_context = "\n".join(chat_history)
    prompt = (
        f"Chat History:\n{history_context}\n\n"
        f"Based on the following information and the chat history, answer the user's question concisely:\n\n"
        f"Information:\n{context}\n\n"
        f"User Question: {query}\n\n"
        f"Answer:"
    )

    # Limit the total prompt length to avoid overwhelming the LLM context window
    max_prompt_length = 8000 # Adjust based on LLM's context window
    if len(prompt) > max_prompt_length:
        prompt = prompt[:max_prompt_length] + "..." # Truncate if too long
        logger.warning(f"[Modular RAG] Prompt truncated to {max_prompt_length} characters.")


    try:
        response = LLM.invoke(prompt)
        return response.content.strip()
    except Exception as e:
        logger.error(f"[Modular RAG] Error generating modular RAG response: {e}")
        return "An error occurred while generating the answer."

# --- Agentic RAG Execution ---
# Modify agentic_rag to use the initialized agent object and incorporate chat history
def agentic_rag(query: str, chat_history: List[str], max_rounds: int = 3, threshold: float = 0.65):
    """
    Escalates to agent loop: uses agent with tools (retriever, web search, summarizer)
    to find and synthesize information. CRAG is used before entering this path.
    """
    logger.info(f"[Agentic RAG] Starting agentic execution for query: {query}")

    # Ensure agent object is initialized globally
    if 'agent' not in globals():
         raise ValueError("LangChain agent is not initialized.")


    # Incorporate chat history into the prompt for the agent
    history_context = "\n".join(chat_history)
    agent_prompt = (
        f"Chat History:\n{history_context}\n\n"
        f"You are an agricultural expert answering a user query. Use the available tools "
        f"(VectorRetriever for internal knowledge, WebSearch for real-time data, Summarizer to process findings) "
        f"to find relevant information and answer the following question:\n\n{query}\n" # Use the original query here
        f"If you find relevant information from the VectorRetriever, use the Summarizer tool to process it."
        f"If the VectorRetriever does not provide enough information, try the WebSearch tool."
        f"Once you have gathered enough evidence, provide a comprehensive answer."
    )
    logger.info(f"[Agentic RAG] Agent prompt with history:\n{agent_prompt[:500]}...") # Print first 500 chars of prompt


    try:
        # Run the agent with the prepared prompt
        logger.info("[Agentic RAG] Running LangChain agent.")
        # Pass the agent_prompt as the sole argument to agent.run()
        final_answer = agent.run(agent_prompt)
        logger.info("[Agentic RAG] Agent execution complete.")
        return final_answer
    except Exception as e:
        logger.error(f"[Agentic RAG] Error during agent execution: {e}")
        # If the agent fails, provide a fallback message
        return "An error occurred while using the agent to find information."

# Need a placeholder for modular_rag if it's not defined elsewhere
# It should take the query and the list of scored chunks (Retrieval object)
# Add chat_history parameter
if 'modular_rag' not in globals():
    def modular_rag(query: str, retrieval: Retrieval, chat_history: List[str]) -> str:
        logger.info(f"[Modular RAG] Processing query: {query}")
        # Simple synthesis using the top chunks from retrieval
        context = "\n".join([chunk.text for chunk in retrieval.chunks[:5]]) # Use top 5 chunks
        # Incorporate chat history into the prompt (basic example)
        history_context = "\n".join(chat_history)
        prompt = (
            f"Chat History:\n{history_context}\n\n"
            f"Based on the following information and the chat history, answer the following question:\n\n"
            f"Question: {query}\n\n"
            f"Context:\n{context}\n\n"
            f"Answer:"
        )

        try:
            response = LLM.invoke(prompt)
            return response.content.strip()
        except Exception as e:
            logger.error(f"[Modular RAG] Synthesis failed: {e}")
            return f"Error generating answer: {e}"

# Need the Query dataclass defined for retrieve_with_expansion
@dataclass
class Query:
    id: str
    text: str
    metadata: Dict[str, Any]

# Need the Retrieval dataclass defined for retrieve_with_expansion
# Assuming Retrieval is defined in ztDR1ktk5ySG or other relevant cell and is globally available
# If not, define it here:
# @dataclass
# class Retrieval:
#     query_id: str
#     chunks: List[Chunk]
#     metadata: Dict[str, Any]

# Need the Chunk dataclass defined for retrieve_with_expansion
# Assuming Chunk is defined in fd_xUIwXnbbb or other relevant cell and is globally available
# If not, define it here:
# @dataclass
# class Chunk:
#     doc_id: str
#     chunk_id: str
#     text: str
#     metadata: Dict[str, Any]
#     embedding: Optional[List[float]] = None

import datetime
from typing import Dict, Any
# Import the Retrieval and Chunk dataclasses if not already imported
# from .your_module import Retrieval, Chunk # Assuming they are in a module
# Since they are defined in other notebook cells, they should be globally available after execution

# Ensure the functions from Hm5v3n3R14nY are available (they are after executing that cell)
# from Hm5v3n3R14nY import hyde_expand, generate_query_embeddings, multi_query_expand, retrieve_with_expansion, Query, Retrieval, Chunk, LLM # Assuming export or global scope

def format_final_output(
    answer: str,
    query: str,
    retrieved_docs, # This will be a list of Chunk objects
    confidence: float,
    decision_path: str
) -> Dict[str, Any]:
    """
    Final output formatting for farmer-friendly assistant.
    Includes provenance, citations, confidence, and timestamp.
    """

    # Collect provenance (source + metadata) from Chunk objects
    provenance = []
    for chunk in retrieved_docs:
        provenance.append({
            "source": chunk.metadata.get("source", "N/A"),
            "page": chunk.metadata.get("page", None),
            "crop": chunk.metadata.get("crop", None),
            "region": chunk.metadata.get("region", None),
            "season": chunk.metadata.get("season", None),
            "doc_id": chunk.metadata.get("doc_id", "N/A"),
            "chunk_id": chunk.chunk_id,
            "fusion_score": chunk.metadata.get("fusion_score", None), # Include fusion score if available
            "crag_score": chunk.metadata.get("crag_score", None), # Include CRAG score
        })

    return {
        "query": query,
        "decision_path": decision_path,   # modular vs agentic
        "answer": f"🌱 Farmer Guidance:\n\n{answer}",
        "confidence": round(confidence, 3),
        "citations": provenance,
        "timestamp": datetime.datetime.now().isoformat(),
        "notes": "This is advisory guidance. Please consult local agricultural experts for critical decisions."
    }

# --- Example Unified Flow ---
"""
def rag_pipeline(query: str, vectorstore: Chroma, threshold: float = 0.65):
    Unified entrypoint: runs retrieval → CRAG → modular or agentic RAG → final formatted output.

    # Ensure LLM and embedding model are accessible
    if 'LLM' not in globals():
         raise ValueError("LLM is not initialized.")
    if 'EMB' not in globals():
         raise ValueError("Embedding model is not initialized.")
    if 'VSTORE' not in globals(): # retrieve_with_expansion needs VSTORE
         raise ValueError("Chroma persistent client (VSTORE) is not initialized.")
    if 'retrieve_with_expansion' not in globals(): # Ensure the function is available
         raise ValueError("retrieve_with_expansion function is not defined. Please run cell Hm5v3n3R14nY.")


    # Step 1: Initial retrieval using expansion techniques
    # retrieve_with_expansion expects a Query object
    query_obj = Query(id="user_query", text=query, metadata={})
    # retrieve_with_expansion uses VSTORE globally and EMB globally
    retrieval = retrieve_with_expansion(query_obj, top_k=8) # Use retrieve_with_expansion


    # RRF fusion is handled internally by retrieve_with_expansion,
    # so we can skip the explicit reciprocal_rank_fusion call here.
    # The 'retrieval' object returned by retrieve_with_expansion already contains
    # the aggregated and potentially fused chunks.

    # Step 2: CRAG scoring
    # Pass the retrieval object (containing Chunk objects with embeddings) and the LLM
    # retrieve_with_expansion should ideally return Chunks with embeddings,
    # or CRAG scoring needs to handle embedding generation if missing.
    # Let's assume retrieve_with_expansion returns Chunks with embeddings for now.
    # If not, CRAG scoring will need to generate them or retrieve them.
    # Based on the definition of retrieve_with_expansion, it creates Chunk objects
    # directly from Chroma results, which might not include embeddings.
    # CRAG scoring function (in Ptt3QnZ74O3q) *does* attempt to generate embeddings
    # if chunk.embedding is None. This seems to be the intended flow.

    decision, scored_retrieval = crag_scoring(retrieval, query, LLM, threshold=threshold)


    if decision == "modular":
        logger.info("[Pipeline] Following Modular RAG path")
        # modular_rag needs to accept the query and the scored_retrieval (list of Chunk objects)
        # Assuming modular_rag is defined elsewhere and can handle Chunk objects
        if 'modular_rag' in globals():
            answer = modular_rag(query, scored_retrieval)
        else:
            logger.error("modular_rag function is not defined.")
            answer = "Error: Modular RAG function is not available."

    else: # decision == "agentic"
        logger.info("[Pipeline] Following Agentic RAG path")
        # agentic_rag function will handle its own retrieval and scoring loop
        # It needs access to LLM, vectorstore and EMB (embedding model)
        # Passing threshold as well
        if 'agentic_rag' in globals():
             # The agentic_rag function should return the final answer string
             answer = agentic_rag(query, threshold=threshold)
        else:
             logger.error("agentic_rag function is not defined.")
             answer = "Error: Agentic RAG function is not available."


    # Step 3: Confidence comes from CRAG score of top doc (simplified here)
    # Use the scores from the scored_retrieval object
    confidence = scored_retrieval.chunks[0].metadata.get("crag_score", 0) if scored_retrieval and scored_retrieval.chunks else 0.0


    # Step 4: Final formatted response
    # Pass the list of scored Chunk objects
    return format_final_output(
        answer=answer,
        query=query,
        retrieved_docs=scored_retrieval.chunks if scored_retrieval else [], # Pass the scored chunks
        confidence=confidence,
        decision_path=decision
    )
"""
# Need a placeholder for modular_rag if it's not defined elsewhere
# It should take the query and the list of scored chunks (Retrieval object)
# Add chat_history parameter
if 'modular_rag' not in globals():
    def modular_rag(query: str, retrieval: Retrieval, chat_history: List[str]) -> str:
        logger.info(f"[Modular RAG] Processing query: {query}")
        # Simple synthesis using the top chunks from retrieval
        context = "\n".join([chunk.text for chunk in retrieval.chunks[:5]]) # Use top 5 chunks
        # Incorporate chat history into the prompt (basic example)
        history_context = "\n".join(chat_history)
        prompt = (
            f"Chat History:\n{history_context}\n\n"
            f"Based on the following information and the chat history, answer the following question:\n\n"
            f"Question: {query}\n\n"
            f"Context:\n{context}\n\n"
            f"Answer:"
        )

        try:
            response = LLM.invoke(prompt)
            return response.content.strip()
        except Exception as e:
            logger.error(f"[Modular RAG] Synthesis failed: {e}")
            return f"Error generating answer: {e}"

# Need the Query dataclass defined for retrieve_with_expansion
@dataclass
class Query:
    id: str
    text: str
    metadata: Dict[str, Any]

# Need the Retrieval dataclass defined for retrieve_with_expansion
# Assuming Retrieval is defined in ztDR1ktk5ySG or other relevant cell and is globally available
# If not, define it here:
# @dataclass
# class Retrieval:
#     query_id: str
#     chunks: List[Chunk]
#     metadata: Dict[str, Any]

# Need the Chunk dataclass defined for retrieve_with_expansion
# Assuming Chunk is defined in fd_xUIwXnpbb or other relevant cell and is globally available
# If not, define it here:
# @dataclass
# class Chunk:
#     doc_id: str
#     chunk_id: str
#     text: str
#     metadata: Dict[str, Any]
#     embedding: Optional[List[float]] = None

import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any
import hashlib
import time
from pathlib import Path
from langchain_community.vectorstores import Chroma # Need this import for the return type
from langchain_core.embeddings import Embeddings
import logging

logger = logging.getLogger("rag")

# Stable directories
DATA_DIR = Path("data")
CACHE_DIR = Path("cache")
OUTPUT_DIR = Path("output")
CACHE_DIR.mkdir(parents=True, exist_ok=True)
DATA_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Initialize embedding model
# Wrap SentenceTransformer with a class that has an embed_documents method
class SentenceTransformerEmbeddings(Embeddings):
    def __init__(self, model_name: str):
        self.model = SentenceTransformer(model_name)
        logger.info(f"Initialized embedding model: {model_name}")


    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        embeddings = self.model.encode(texts, batch_size=32, show_progress_bar=False).tolist()
        logger.info(f"Generated {len(embeddings)} embeddings for {len(texts)} texts.") # Diagnostic print
        return embeddings


    def embed_query(self, text: str) -> List[float]:
        embedding = self.model.encode([text], show_progress_bar=False).tolist()[0]
        logger.info("Generated embedding for query.") # Diagnostic print
        return embedding


EMB = SentenceTransformerEmbeddings("sentence-transformers/all-MiniLM-L6-v2")


def get_embedding_version(model_name: str) -> str:
    # Use the class name for versioning
    return hashlib.sha256(model_name.encode("utf-8")).hexdigest()[:8]

# Initialize Chroma persistent client here
VSTORE = chromadb.PersistentClient(path=str(CACHE_DIR / "chroma"))
logger.info(f"Initialized Chroma persistent client at {CACHE_DIR / 'chroma'}")


def generate_embeddings(chunks: List[Chunk], collection_name: str = "agri_docs") -> Chroma:
    """
    Generates embeddings for chunks, adds them to a ChromaDB collection,
    and returns a LangChain Chroma wrapper around the collection.
    """
    # Get or create the collection using the persistent client
    collection = VSTORE.get_or_create_collection(
        name=collection_name,
        metadata={"embedding_version": get_embedding_version(EMB.__class__.__name__)}
    )
    logger.info(f"Accessed or created Chroma collection: '{collection_name}'")


    if not chunks:
        logger.warning("No chunks to generate embeddings for. Returning LangChain Chroma wrapper for empty collection.")
        # Return the LangChain wrapper even if no chunks to ensure the correct type is returned
        vectorstore = Chroma(
            client=VSTORE,
            collection_name=collection_name,
            embedding_function=EMB,
            persist_directory=str(CACHE_DIR / "chroma")
        )
        return vectorstore


    texts = [chunk.text for chunk in chunks]
    ids = [chunk.chunk_id for chunk in chunks]
    metadatas = [chunk.metadata for chunk in chunks]

    # Generate embeddings explicitly
    embeddings = EMB.embed_documents(texts)

    if not embeddings:
         logger.warning("Embedding generation returned an empty list. Cannot add to Chroma.")
         # Return the LangChain wrapper for the potentially empty collection
         vectorstore = Chroma(
             client=VSTORE,
             collection_name=collection_name,
             embedding_function=EMB,
             persist_directory=str(CACHE_DIR / "chroma")
         )
         return vectorstore


    # Add data to the collection in batches
    batch_size = 100  # Adjust batch size as needed
    for i in range(0, len(chunks), batch_size):
        batch_texts = texts[i : i + batch_size]
        batch_ids = ids[i : i + batch_size]
        batch_metadatas = metadatas[i : i + batch_size]
        batch_embeddings = embeddings[i : i + batch_size]


        # Check if batch_embeddings is not empty before upserting
        if batch_embeddings:
            collection.upsert(
                documents=batch_texts,
                embeddings=batch_embeddings,
                ids=batch_ids,
                metadatas=batch_metadatas
            )
            logger.info(f"Upserted batch {i//batch_size} ({len(batch_ids)} chunks)")
        else:
             logger.warning(f"Skipping upsert for batch {i//batch_size} as it contains no embeddings.")


    logger.info(f"Generated and stored {len(chunks)} embeddings in collection '{collection_name}'")

    # Return the LangChain Chroma wrapper
    vectorstore = Chroma(
        client=VSTORE,
        collection_name=collection_name,
        embedding_function=EMB,
        persist_directory=str(CACHE_DIR / "chroma")
    )
    return vectorstore


# The main execution logic in this cell
cfg = {}
docs = ingest_and_parse_documents(DATA_DIR, cfg)
print(f"Number of documents collected: {len(docs)}") # Diagnostic print
chunks = chunk_documents(docs, chunk_size=100, overlap=20)
print(f"Number of chunks created: {len(chunks)}") # Diagnostic print

# Generate embeddings and populate the Chroma collection, and get the LangChain wrapper
vectorstore = generate_embeddings(chunks, collection_name="agri_docs")


print("ChromaDB populated and LangChain vectorstore wrapper initialized.")

# Ensure the global vectorstore is the LangChain wrapper
# This assignment should now be correct as generate_embeddings returns the wrapper
# (Redundant assignment but clarifies intent)
# global vectorstore # Not needed if assigned directly
# vectorstore = vectorstore # No-op, just for clarity if needed

import chromadb
from chromadb.config import Settings
from pathlib import Path

# Assuming CACHE_DIR is defined elsewhere (e.g., in cell xtTL4IXVZNDZ)
# Initialize a PersistentClient to interact directly with the Chroma database
try:
    client = chromadb.PersistentClient(path=str(CACHE_DIR / "chroma"))
    print("Available collections:", client.list_collections())
except NameError:
    print("Error: CACHE_DIR is not defined. Please run the cell that defines CACHE_DIR first.")
except Exception as e:
    print(f"An error occurred: {e}")

import chromadb
from chromadb.config import Settings
from pathlib import Path

# Assuming 'vectorstore' is already initialized and populated
# by the execution of the previous cells, which now uses LangChain's Chroma.

# You can query the vectorstore object directly
# The LangChain Chroma object doesn't have a direct 'get' method like the raw chromadb client
# We can use 'similarity_search' or other LangChain retrieval methods.
# To see the content, we can fetch all documents (with a limit for large collections)
# This requires accessing the underlying client from the vectorstore object.

try:
    # Access the underlying chromadb client
    # Note: This might depend on the specific LangChain Chroma implementation details
    # A more robust way would be to re-initialize a persistent client here if needed
    # However, let's try accessing the client assumed to be within the vectorstore object

    # Re-initializing a persistent client is safer to inspect the database directly
    client = chromadb.PersistentClient(path=str(CACHE_DIR / "chroma"))
    collection = client.get_collection("agri_docs") # Use the collection name used in generate_embeddings

    results = collection.get(limit=3, include=["documents", "metadatas", "embeddings"]) # Removed "ids" from include


    print("IDs:", results.get("ids", "Not included")) # Access IDs separately if needed
    print("Texts:", results.get("documents", "Not included"))
    print("Metadata:", results.get("metadatas", "Not included"))

    embeddings_list = results.get("embeddings")

    if embeddings_list is not None and len(embeddings_list) > 0:  # explicit check
      print("Embedding vector length:", len(embeddings_list[0]))
      print("First 10 dims of first vector:", embeddings_list[0][:10])
    else:
      print("No embeddings found in the results.")


except Exception as e:
    print(f"An error occurred: {e}")
    print("Could not access collection directly via the vectorstore object's internal client.")
    print("If you need to directly inspect the ChromaDB, you might need to initialize a separate PersistentClient.")

"""# Task
Implement a multi-user chat history feature for the RAG pipeline using `sqlite3`. The chat history should be stored in a database named "rag.db" with a table schema including `session_id`, `user_query`, and `AIanswer`. Implement functions to connect to the database, create the table, insert new chat entries, and retrieve chat history based on a unique `session_id` generated using `uuid`. Modify the existing RAG pipeline to utilize these functions for managing chat history.

## Initialize sqlite database

### Subtask:
Create a function to establish a connection to the SQLite database and ensure the necessary table for storing chat history exists.

**Reasoning**:
The subtask requires creating a function to initialize the SQLite database and create the chat history table. I will define the `init_db` function, connect to the database, create a cursor, execute the CREATE TABLE SQL statement, commit the changes, and close the connection.
"""



"""**Reasoning**:
The `init_db` function has been defined as required by the subtask. I will now call this function to initialize the database and verify its execution.


"""



"""## Generate session ids

### Subtask:
Implement a mechanism to generate unique session IDs for each user or chat session.

**Reasoning**:
Implement a function to generate unique session IDs using `uuid`.
"""

import uuid

def generate_session_id() -> str:
    """Generates a unique session ID."""
    return str(uuid.uuid4())

"""## Modify `rag pipeline` to use session ids

### Subtask:
Update the `rag_pipeline` function to accept a session ID and retrieve the relevant chat history from the database.

**Reasoning**:
Modify the `rag_pipeline` function signature to include a `session_id: str` parameter, call the `get_chat_history` function (placeholder for now) within `rag_pipeline`, and pass the retrieved chat history to the modular and agentic RAG functions.
"""

import datetime
from typing import Dict, Any, List


def format_final_output(
    answer: str,
    query: str,
    retrieved_docs, # This will be a list of Chunk objects
    confidence: float,
    decision_path: str
) -> Dict[str, Any]:
    """
    Final output formatting for farmer-friendly assistant.
    Includes provenance, citations, confidence, and timestamp.
    """

    # Collect provenance (source + metadata) from Chunk objects
    provenance = []
    for chunk in retrieved_docs:
        provenance.append({
            "source": chunk.metadata.get("source", "N/A"),
            "page": chunk.metadata.get("page", None),
            "crop": chunk.metadata.get("crop", None),
            "region": chunk.metadata.get("region", None),
            "season": chunk.metadata.get("season", None),
            "doc_id": chunk.metadata.get("doc_id", "N/A"),
            "chunk_id": chunk.chunk_id,
            "fusion_score": chunk.metadata.get("fusion_score", None), # Include fusion score if available
            "crag_score": chunk.metadata.get("crag_score", None), # Include CRAG score
        })

    return {
        "query": query,
        "decision_path": decision_path,   # modular vs agentic
        "answer": f"🌱 Farmer Guidance:\n\n{answer}",
        "confidence": round(confidence, 3),
        "citations": provenance,
        "timestamp": datetime.datetime.now().isoformat(),
        "notes": "This is advisory guidance. Please consult local agricultural experts for critical decisions."
    }

# --- Example Unified Flow ---
# Ensure session_id and chat_history parameters are included in the function signature
def rag_pipeline(query: str, vectorstore: Chroma, session_id: str, threshold: float = 0.35):
    """
    Unified entrypoint: runs retrieval → CRAG → modular or agentic RAG → final formatted output.
    Accepts a session ID to retrieve and utilize chat history.
    """
    # Ensure LLM and embedding model are accessible
    if 'LLM' not in globals():
         raise ValueError("LLM is not initialized.")
    if 'EMB' not in globals():
         raise ValueError("Embedding model is not initialized.")
    if 'VSTORE' not in globals(): # retrieve_with_expansion needs VSTORE
         raise ValueError("Chroma persistent client (VSTORE) is not initialized.")
    if 'retrieve_with_expansion' not in globals(): # Ensure the function is available
         raise ValueError("retrieve_with_expansion function is not defined. Please run cell Hm5v3n3n14nY.")

    # Retrieve chat history using the session ID
    chat_history = get_chat_history(session_id) # Call the placeholder function


    # Step 1: Initial retrieval using expansion techniques
    # retrieve_with_expansion expects a Query object
    query_obj = Query(id=session_id, text=query, metadata={}) # Use session_id as query_id
    # retrieve_with_expansion uses VSTORE globally and EMB globally
    retrieval = retrieve_with_expansion(query_obj, top_k=8) # Use retrieve_with_expansion


    # RRF fusion is handled internally by retrieve_with_expansion,
    # so we can skip the explicit reciprocal_rank_fusion call here.
    # The 'retrieval' object returned by retrieve_with_expansion already contains
    # the aggregated and potentially fused chunks.

    # Step 2: CRAG scoring
    # Pass the retrieval object (containing Chunk objects with embeddings) and the LLM
    # retrieve_with_expansion should ideally return Chunks with embeddings,
    # or CRAG scoring needs to handle embedding generation if missing.
    # Let's assume retrieve_with_expansion returns Chunks with embeddings for now.
    # If not, CRAG scoring will need to generate them or retrieve them.
    # Based on the definition of retrieve_with_expansion, it creates Chunk objects
    # directly from Chroma results, which might not include embeddings.
    # CRAG scoring function (in Ptt3QnZ74O3q) *does* attempt to generate embeddings
    # if chunk.embedding is None. This seems to be the intended flow.

    decision, scored_retrieval = crag_scoring(retrieval, query, LLM, threshold=threshold)


    if decision == "modular":
        logger.info("[Pipeline] Following Modular RAG path")
        # modular_rag needs to accept the query and the scored_retrieval (list of Chunk objects)
        # Assuming modular_rag is defined elsewhere and can handle Chunk objects
        if 'modular_rag' in globals():
            # Pass chat_history to modular_rag
            answer = modular_rag(query, scored_retrieval, chat_history)
        else:
            logger.error("modular_rag function is not defined.")
            answer = "Error: Modular RAG function is not available."

    else: # decision == "agentic"
        logger.info("[Pipeline] Following Agentic RAG path")
        # agentic_rag function will handle its own retrieval and scoring loop
        # It needs access to LLM, vectorstore and EMB (embedding model)
        # Passing threshold and chat_history using keyword arguments
        if 'agentic_rag' in globals():
             # The agentic_rag function should return the final answer string
             # Pass chat_history and threshold as keyword arguments
             answer = agentic_rag(query, chat_history=chat_history, threshold=threshold)
        else:
             logger.error("agentic_rag function is not defined.")
             answer = "Error: Agentic RAG function is not available."


    # Step 3: Confidence comes from CRAG score of top doc (simplified here)
    # Use the scores from the scored_retrieval object
    confidence = scored_retrieval.chunks[0].metadata.get("crag_score", 0) if scored_retrieval and scored_retrieval.chunks else 0.0


    # Step 4: Final formatted response
    # Pass the list of scored Chunk objects
    add_chat_entry(session_id, query, answer) # Add chat entry to database
    return format_final_output(
        answer=answer,
        query=query,
        retrieved_docs=scored_retrieval.chunks if scored_retrieval else [], # Pass the scored chunks
        confidence=confidence,
        decision_path=decision
    )


# Need a placeholder for modular_rag if it's not defined elsewhere
# It should take the query and the list of scored chunks (Retrieval object)
# Add chat_history parameter
if 'modular_rag' not in globals():
    def modular_rag(query: str, retrieval: Retrieval, chat_history: List[str]) -> str:
        logger.info(f"[Modular RAG] Processing query: {query}")
        # Simple synthesis using the top chunks from retrieval
        context = "\n".join([chunk.text for chunk in retrieval.chunks[:5]]) # Use top 5 chunks
        # Incorporate chat history into the prompt (basic example)
        history_context = "\n".join(chat_history)
        prompt = (
            f"Chat History:\n{history_context}\n\n"
            f"Based on the following information and the chat history, answer the following question:\n\n"
            f"Question: {query}\n\n"
            f"Context:\n{context}\n\n"
            f"Answer:"
        )

        try:
            response = LLM.invoke(prompt)
            return response.content.strip()
        except Exception as e:
            logger.error(f"[Modular RAG] Synthesis failed: {e}")
            return f"Error generating answer: {e}"

# Need the Query dataclass defined for retrieve_with_expansion
@dataclass
class Query:
    id: str
    text: str
    metadata: Dict[str, Any]

# Need the Retrieval dataclass defined for retrieve_with_expansion
# Assuming Retrieval is defined in ztDR1ktk5ySG or other relevant cell and is globally available
# If not, define it here:
# @dataclass
# class Retrieval:
#     query_id: str
#     chunks: List[Chunk]
#     metadata: Dict[str, Any]

# Need the Chunk dataclass defined for retrieve_with_expansion
# Assuming Chunk is defined in fd_xUIwXnbbb or other relevant cell and is globally available
# If not, define it here:
# @dataclass
# class Chunk:
#     doc_id: str
#     chunk_id: str
#     text: str
#     metadata: Dict[str, Any]
#     embedding: Optional[List[float]] = None

# Need the agentic_rag placeholder to also accept chat_history
if 'agentic_rag' not in globals():
    def agentic_rag(query: str, chat_history: List[str], max_rounds: int = 3, threshold: float = 0.65):
        logger.info(f"[Agentic RAG Placeholder] Starting agentic loop for query: {query}")
        # Format chat history for the agent prompt
        history_context = "\n".join(chat_history)
        agent_prompt = (
            f"Chat History:\n{history_context}\n\n"
            f"You are an agricultural expert answering a user query. Use the available tools "
            f"(VectorRetriever for internal knowledge, WebSearch for real-time data, Summarizer to process findings) "
            f"to find relevant information and answer the following question:\n\n{query}\n" # Use the original query here
            f"If you find relevant information from the VectorRetriever, use the Summarizer tool to process it."
            f"If the VectorRetriever does not provide enough information, try the WebSearch tool."
            f"Once you have gathered enough evidence, provide a comprehensive answer."
        )
        # This is a placeholder, actual agentic logic would go here.
        # For now, return a simple response including the modified prompt structure.
        logger.info(f"[Agentic RAG Placeholder] Agent prompt with history:\n{agent_prompt[:500]}...") # Print first 500 chars of prompt
        return f"Agentic path triggered for query: '{query}'. Chat history length: {len(chat_history)}. Agent prompt includes history."

"""## Update chat history storage

### Subtask:
Modify the process of storing chat history to insert new user queries and AI responses into the SQLite database, associated with the current session ID.

**Reasoning**:
Create a new function `add_chat_entry` to insert chat entries into the SQLite database and modify the `rag_pipeline` function to call it after an answer is generated, fulfilling the current subtask requirements.
"""



"""## Update chat history retrieval

### Subtask:
Update the `get_chat_history` function to retrieve chat history from the SQLite database based on the provided session ID.

**Reasoning**:
Implement the `get_chat_history` function to retrieve chat history from the SQLite database based on the provided session ID, format it, and return the formatted history.
"""



from supabase import create_client, Client
from typing import List

# --- Supabase Connection Setup ---
SUPABASE_URL = "https://pysflltbiyglnwyxdrkp.supabase.co"  # from project settings
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InB5c2ZsbHRiaXlnbG53eXhkcmtwIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1NTUxMjE4OSwiZXhwIjoyMDcxMDg4MTg5fQ.qO2bKg0qptka2CRYjLAKE7j5Aa_O2PdJxZrXI5hI-_c"  # use service_role, NOT anon if you need insert

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# --- Initialize Database ---
def init_db():
    """
    Creates the chat_history table if it doesn't exist.
    ⚠️ Supabase does not auto-create tables from client side,
       you should run this SQL once in Supabase SQL Editor.
    """
    try:
        response = supabase.table("chat_history").select("*").limit(1).execute()
        print("✅ Supabase connected. Table `chat_history` exists.")
    except Exception as e:
        print("⚠️ You need to create `chat_history` table manually in Supabase SQL Editor.")
        print("Run this SQL once:")
        print("""
        CREATE TABLE IF NOT EXISTS chat_history (
            id BIGSERIAL PRIMARY KEY,
            session_id TEXT NOT NULL,
            user_query TEXT,
            ai_answer TEXT
        );
        """)
        print(f"Error: {e}")

# --- Insert Chat Entry ---
def add_chat_entry(session_id: str, user_query: str, ai_answer: str):
    """Inserts a new chat entry into Supabase."""
    try:
        data = {
            "session_id": session_id,
            "user_query": user_query,
            "ai_answer": ai_answer
        }
        supabase.table("chat_history").insert(data).execute()
        print(f"✅ Added chat entry for session ID: {session_id}")
    except Exception as e:
        print(f"Supabase error adding chat entry: {e}")

# --- Retrieve Chat History ---
def get_chat_history(session_id: str) -> List[str]:
    """Fetches chat history for a given session from Supabase."""
    chat_history = []
    try:
        response = supabase.table("chat_history") \
            .select("user_query, ai_answer") \
            .eq("session_id", session_id) \
            .execute()

        if response.data:
            for row in response.data:
                chat_history.append(f"User: {row['user_query']}")
                chat_history.append(f"AI: {row['ai_answer']}")
            print(f"✅ Retrieved {len(chat_history)//2} turns for session {session_id}")
        else:
            print("No chat history found.")
    except Exception as e:
        print(f"Supabase error retrieving chat history: {e}")
    return chat_history

init_db()

"""## Integrate with example usage

### Subtask:
Modify the example usage to generate a session ID and pass it to the `rag_pipeline`, as well as handle the updating and retrieval of chat history from the database.

**Reasoning**:
Modify the example usage to generate a session ID, pass it to the rag_pipeline, and handle the updating and retrieval of chat history from the database as required by the subtask.
"""

# Example usage:
query = "How to grow mango in bihar?"

